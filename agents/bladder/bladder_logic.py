"""
DR_BLADDER Agent - Core Logic Module
ë°©ê´‘ì•” ê´€ë ¨ ì˜ë£Œ ì§ˆë¬¸ ë¶„ì„ ë° ì‘ë‹µ ìƒì„±
"""

import ollama
import json
import logging
import sys
from typing import Dict, Any, Optional
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from agents.shared.vector_db import get_vector_db

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BladderAnalyzer:
    """ë°©ê´‘ì•” ì „ë¬¸ ì˜ë£Œ ë¶„ì„ê¸°"""
    
    def __init__(self, model_name: str = "gemma3:4b"):
        """
        ì´ˆê¸°í™”
        Args:
            model_name: Ollama ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸: gemma3:4b)
        """
        self.model_name = model_name
        self.system_prompt = self._get_system_prompt()
        
        # ë²¡í„° DB ì´ˆê¸°í™”
        try:
            self.vector_db = get_vector_db()
            logger.info("Vector DB initialized for bladder guidelines")
        except Exception as e:
            logger.warning(f"Vector DB initialization failed: {e}")
            self.vector_db = None
        
    def _get_system_prompt(self) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜"""
        return """You are DR_BLADDER, a specialized medical AI assistant focused on bladder cancer.
        
Your expertise includes:
- Bladder cancer diagnosis and staging
- Treatment options (BCG therapy, chemotherapy, surgical interventions)
- Risk factors and prevention
- Patient care and follow-up protocols
- Latest research and clinical guidelines

Guidelines:
1. Provide evidence-based medical information
2. Use appropriate medical terminology with explanations
3. Consider patient safety and emphasize professional consultation
4. Structure responses clearly with sections when appropriate
5. Include relevant statistics and success rates when available

Remember: Always recommend consultation with healthcare professionals for personal medical decisions."""

    def analyze_bladder_question(self, question: str) -> str:
        """
        ë°©ê´‘ì•” ê´€ë ¨ ì§ˆë¬¸ ë¶„ì„ ë° ì‘ë‹µ ìƒì„±
        
        Args:
            question: ì‚¬ìš©ìì˜ ì˜ë£Œ ì§ˆë¬¸
            
        Returns:
            ì „ë¬¸ì ì¸ ì˜ë£Œ ì‘ë‹µ
            
        Raises:
            Exception: Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨ ë˜ëŠ” ëª¨ë¸ ì˜¤ë¥˜
        """
        try:
            # ì…ë ¥ ê²€ì¦
            if not question or not question.strip():
                return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
            logger.info(f"ë¶„ì„ ì‹œì‘: {question[:50]}...")
            
            # RAG: ê´€ë ¨ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰
            context = ""
            if self.vector_db:
                try:
                    context = self.vector_db.get_context_for_prompt(
                        query=question,
                        source_type="bladder",
                        n_results=3
                    )
                    if context:
                        logger.info("Retrieved context from bladder cancer guidelines")
                    else:
                        logger.info("No relevant context found in guidelines")
                except Exception as e:
                    logger.warning(f"Context retrieval failed: {e}")
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (RAG í†µí•©)
            if context:
                full_prompt = f"""{self.system_prompt}

{context}

Question: {question}

Based on the EAU bladder cancer guidelines provided above, provide a comprehensive medical response:"""
            else:
                full_prompt = f"{self.system_prompt}\n\nQuestion: {question}\n\nProvide a comprehensive medical response:"
            
            # Ollama ëª¨ë¸ í˜¸ì¶œ
            response = ollama.chat(
                model=self.model_name,
                messages=[
                    {
                        'role': 'system',
                        'content': self.system_prompt
                    },
                    {
                        'role': 'user',
                        'content': question
                    }
                ],
                options={
                    'temperature': 0.7,
                    'top_p': 0.9,
                    'max_tokens': 2048
                }
            )
            
            # ì‘ë‹µ ì¶”ì¶œ
            if response and 'message' in response:
                answer = response['message']['content']
                logger.info("ë¶„ì„ ì™„ë£Œ")
                return self._format_response(answer, question)
            else:
                logger.error("ëª¨ë¸ ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜")
                return "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                
        except ollama.ResponseError as e:
            logger.error(f"Ollama ì‘ë‹µ ì˜¤ë¥˜: {e}")
            return f"ëª¨ë¸ ì‘ë‹µ ì˜¤ë¥˜: {str(e)}"
        except Exception as e:
            logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return f"ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _format_response(self, answer: str, question: str) -> str:
        """
        ì‘ë‹µ í¬ë§·íŒ…
        
        Args:
            answer: ì›ë³¸ ì‘ë‹µ
            question: ì›ë³¸ ì§ˆë¬¸
            
        Returns:
            í¬ë§·ëœ ì‘ë‹µ
        """
        # ì‘ë‹µì— í•„ìˆ˜ ê²½ê³  ë¬¸êµ¬ ì¶”ê°€
        disclaimer = "\n\nâš ï¸ **ì˜í•™ì  ì£¼ì˜ì‚¬í•­**: ì´ ì •ë³´ëŠ” êµìœ¡ ëª©ì ìœ¼ë¡œë§Œ ì œê³µë©ë‹ˆë‹¤. ì‹¤ì œ ì§„ë‹¨ê³¼ ì¹˜ë£ŒëŠ” ë°˜ë“œì‹œ ì „ë¬¸ ì˜ë£Œì§„ê³¼ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        
        # ì§ˆë¬¸ ìœ í˜• ë¶„ì„
        if any(keyword in question.lower() for keyword in ['ì§„ë‹¨', 'diagnosis', 'ì¦ìƒ', 'symptom']):
            prefix = "ğŸ“‹ **ì§„ë‹¨ ê´€ë ¨ ì •ë³´**\n\n"
        elif any(keyword in question.lower() for keyword in ['ì¹˜ë£Œ', 'treatment', 'therapy', 'bcg']):
            prefix = "ğŸ’Š **ì¹˜ë£Œ ê´€ë ¨ ì •ë³´**\n\n"
        elif any(keyword in question.lower() for keyword in ['ì˜ˆë°©', 'prevention', 'ìœ„í—˜', 'risk']):
            prefix = "ğŸ›¡ï¸ **ì˜ˆë°© ë° ìœ„í—˜ ìš”ì¸**\n\n"
        else:
            prefix = "ğŸ¥ **ì˜ë£Œ ì •ë³´**\n\n"
        
        return prefix + answer + disclaimer
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        ëª¨ë¸ ì •ë³´ ë°˜í™˜
        
        Returns:
            ëª¨ë¸ ìƒíƒœ ë° ì •ë³´
        """
        try:
            # Ollama ëª¨ë¸ ì •ë³´ ì¡°íšŒ
            models = ollama.list()
            model_info = {
                'model_name': self.model_name,
                'status': 'unknown',
                'size': 'unknown'
            }
            
            for model in models.get('models', []):
                if self.model_name in model.get('name', ''):
                    model_info['status'] = 'available'
                    model_info['size'] = model.get('size', 'unknown')
                    break
            
            return model_info
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                'model_name': self.model_name,
                'status': 'error',
                'error': str(e)
            }
    
    def validate_ollama_connection(self) -> bool:
        """
        Ollama ì„œë²„ ì—°ê²° í™•ì¸
        
        Returns:
            ì—°ê²° ì„±ê³µ ì—¬ë¶€
        """
        try:
            ollama.list()
            logger.info("Ollama ì„œë²„ ì—°ê²° ì„±ê³µ")
            return True
        except Exception as e:
            logger.error(f"Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            return False


# ë‹¨ë… í•¨ìˆ˜ ì¸í„°í˜ì´ìŠ¤ (ê¸°ì¡´ CLI í˜¸í™˜ì„±)
def analyze_bladder_question(question: str, model_name: str = "gemma3:4b") -> str:
    """
    ë°©ê´‘ì•” ì§ˆë¬¸ ë¶„ì„ í•¨ìˆ˜ (ë‹¨ìˆœ ì¸í„°í˜ì´ìŠ¤)
    
    Args:
        question: ë¶„ì„í•  ì˜ë£Œ ì§ˆë¬¸
        model_name: ì‚¬ìš©í•  Ollama ëª¨ë¸
        
    Returns:
        ì˜ë£Œ ì „ë¬¸ ì‘ë‹µ
    """
    analyzer = BladderAnalyzer(model_name=model_name)
    return analyzer.analyze_bladder_question(question)


# í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ
if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    test_question = "What are the main treatment options for bladder cancer?"
    analyzer = BladderAnalyzer()
    
    # ì—°ê²° í™•ì¸
    if analyzer.validate_ollama_connection():
        print("âœ“ Ollama ì„œë²„ ì—°ê²°ë¨")
        print(f"ëª¨ë¸ ì •ë³´: {analyzer.get_model_info()}")
        
        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
        response = analyzer.analyze_bladder_question(test_question)
        print(f"\nì§ˆë¬¸: {test_question}")
        print(f"ì‘ë‹µ:\n{response}")
    else:
        print("âœ— Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨")